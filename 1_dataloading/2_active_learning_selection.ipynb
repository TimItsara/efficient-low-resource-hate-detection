{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e52f643-722a-4c96-8e5c-bb4381a4c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f90b0-69eb-48e4-9607-a2e9395805e1",
   "metadata": {},
   "source": [
    "## Load original train datasets that active learner was applied to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538ba705-5d85-4f6f-bd9e-f8339d4f4d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has21_hi\n",
      "has20_hi\n",
      "for19_pt\n",
      "ken20_en\n",
      "fou18_en\n",
      "ous19_fr\n",
      "has19_hi\n",
      "bas19_es\n",
      "ous19_ar\n",
      "san20_it\n"
     ]
    }
   ],
   "source": [
    "df_dict = dict()\n",
    "\n",
    "PATH = \"../0_data/main/1_clean\"\n",
    "    \n",
    "for dataset in os.listdir(PATH):\n",
    "    for f in glob.glob(f\"{PATH}/{dataset}/train*.csv\"):\n",
    "        if \"dyn21\" not in f and \"ipynb\" not in f:\n",
    "            print(dataset[:8])\n",
    "            df_dict[dataset[:8]] = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11428001-a47c-464e-8ece-948d5f2f8b44",
   "metadata": {},
   "source": [
    "## Merge train datasets with prediction logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24449033-d869-4ce5-b87e-69b80d1ed0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ous19_ar\n",
      "for19_pt\n",
      "san20_it\n",
      "ous19_fr\n",
      "bas19_es\n"
     ]
    }
   ],
   "source": [
    "PATH = \"../0_data/main/2_active_learning\"\n",
    "AL_MODEL = \"xlmt_dyn21_en_20000_rs1\"\n",
    "\n",
    "for dataset in os.listdir(PATH):\n",
    "    print(dataset[:8])\n",
    "    df_dict[dataset[:8]] = df_dict[dataset[:8]].merge(pd.read_csv(f\"{PATH}/{dataset}/{AL_MODEL}.csv\")[[\"prediction\", \"logits\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f6eaf-ab02-4ebb-bf42-94500c3cf5d2",
   "metadata": {},
   "source": [
    "## Create columns for selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b66c15-74aa-4088-8504-b1dffee1a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "for dataset in df_dict:\n",
    "    if \"logits\" in df_dict[dataset].columns:\n",
    "        df_dict[dataset][\"softmax_scores\"] = df_dict[dataset].logits.apply(lambda x: softmax(literal_eval(x)))\n",
    "        df_dict[dataset][\"softmax_diff\"] = df_dict[dataset].softmax_scores.apply(lambda x: abs(x[0]-x[1]))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8031d1-a793-4027-9fea-c5c07041b9ae",
   "metadata": {},
   "source": [
    "## Select based on difference in softmax scores across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2b8e3-030e-4cb7-8e47-daed076dcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS21_HI\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "HAS20_HI\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "FOR19_PT\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "KEN20_EN\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "FOU18_EN\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "OUS19_FR\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "HAS19_HI\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "BAS19_ES\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "OUS19_AR\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "SAN20_IT\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create differently-sized train portions from rest of data\n",
    "\n",
    "N_RANGE = [10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000, 2000]\n",
    "\n",
    "for dataset in df_dict:\n",
    "    print(dataset.upper())\n",
    "    for n in N_RANGE:\n",
    "        print(f\"  saving n = {n} training set (selected by active learning)\")\n",
    "        if \"softmax_diff\" in df_dict[dataset].columns:\n",
    "            export_dict = df_dict[dataset].sort_values(\"softmax_diff\")[[\"text\", \"label\"]][:n]\n",
    "        for file in glob.glob(f\"../0_data/main/2_active_learning/{dataset}*\"):\n",
    "            export_dict.to_csv(f\"{file}/train/train_{n}_al.csv\",index=False)\n",
    " \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428773b4-7371-4cc9-9253-60e254287b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>prediction</th>\n",
       "      <th>logits</th>\n",
       "      <th>softmax_scores</th>\n",
       "      <th>softmax_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user L‚Äô Italia ne ha gi√† accolti fin troppo di migranti: Macron apra come si deve le sue frontiere ai migranti e non con il contagocceüò§ü§î</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.88551676, 0.7890035)</td>\n",
       "      <td>[0.15782244096304723, 0.8421775590369528]</td>\n",
       "      <td>0.684355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La sinistra vicina ai pi√π deboli , non operai e gente comune , ma immigrati d'ogni sorta, ritiene giusto violare la legge.  E a noi ci fanno chiudere le piccole aziende per cavilli burocratici. Un grazie da chi non si √® suicidato ma quasi . #Riace #MimmoLucano #Saviano http</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.9222206, 0.8876971)</td>\n",
       "      <td>[0.14064807272758129, 0.8593519272724188]</td>\n",
       "      <td>0.718704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Io sono #Desiree Sono stata stuprata da un branco di #Clandestini  Sono stata lasciata da loro li a morire Sono stata stuprata anche da morta Sono #DesireeMariottini non ho pi√π un futuro per colpa di un gruppo di #immigrati e sono stata violentata anche da un #magistrato italiano http</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>(-1.284241, 1.1302452)</td>\n",
       "      <td>[0.08207470049519884, 0.9179252995048012]</td>\n",
       "      <td>0.835851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un altra conquista dei #PDüëÄ dare le case popolari in mano ai #Rom .üôà Risultato : #degrado e #sopprusi Si ringrazia il sindaco #Salah ü§¶‚Äç‚ôÇÔ∏è http</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.16130793, 0.031392984)</td>\n",
       "      <td>[0.451973296804215, 0.548026703195785]</td>\n",
       "      <td>0.096053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[marioafrica]: RT filippobubbico: Il benvenuto dell'Italia a #migranti. Tra oggi e domani 400 arrivi per #corridoi‚Ä¶ http</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.9738463, -1.1099527)</td>\n",
       "      <td>[0.8893185264132982, 0.11068147358670168]</td>\n",
       "      <td>0.778637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>I PDIOTI PER RIEPIRE LA PIAZZA INGAGGIANO TUTTI I MIGRANTI AFRICANI CLANDESTINI IN ITALIA  COSI PDIOTI +CLANDESTINI  69 PERSONE ü§£ü§£ü§£üòÇüòÇüòÇüòÅüòÅüòÅ AUGURI GRANDI CAZZARI COME IL BOSS CAZZARO RENZI WM5S W NUOVO GOVERNO</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.50333995, 0.3105694)</td>\n",
       "      <td>[0.3070580631467035, 0.6929419368532965]</td>\n",
       "      <td>0.385884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>@user dica alla Fedeli che sistemati questi rom altri ne arriveranno a sfruttare la pieta' e non finiremo mai e per gl'italiani niente</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>(-2.8639174, 2.5761027)</td>\n",
       "      <td>[0.004320647049271328, 0.9956793529507286]</td>\n",
       "      <td>0.991359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5597</th>\n",
       "      <td>Gli islamici devono adeguarsi alle tradizioni popolari e religiose dell'Italia!!! Non il contrario. http</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>(1.1118305, -1.1924204)</td>\n",
       "      <td>[0.9092284853060056, 0.09077151469399444]</td>\n",
       "      <td>0.818457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5598</th>\n",
       "      <td>@user Pensi che questo √© quello che aspetta alle donne con la venuta dell'islam. Maometto era anche pedofilo. Perch√© la sinistra non da dei medievali a loro? Sinistra di gente accecata che corre verso un burrone.</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>(-1.2798216, 1.0581983)</td>\n",
       "      <td>[0.08802273721974115, 0.9119772627802588]</td>\n",
       "      <td>0.823955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5599</th>\n",
       "      <td>@user  puoi anche attivare la vibrazione, infilartelo nel culo e andare a fare in culo insieme ai tuoi amici migranti</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>(-2.1546922, 1.8866277)</td>\n",
       "      <td>[0.017270740338142854, 0.9827292596618572]</td>\n",
       "      <td>0.965459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5600 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                               text  \\\n",
       "0                                                                                                                                                         @user L‚Äô Italia ne ha gi√† accolti fin troppo di migranti: Macron apra come si deve le sue frontiere ai migranti e non con il contagocceüò§ü§î   \n",
       "1                La sinistra vicina ai pi√π deboli , non operai e gente comune , ma immigrati d'ogni sorta, ritiene giusto violare la legge.  E a noi ci fanno chiudere le piccole aziende per cavilli burocratici. Un grazie da chi non si √® suicidato ma quasi . #Riace #MimmoLucano #Saviano http   \n",
       "2     Io sono #Desiree Sono stata stuprata da un branco di #Clandestini  Sono stata lasciata da loro li a morire Sono stata stuprata anche da morta Sono #DesireeMariottini non ho pi√π un futuro per colpa di un gruppo di #immigrati e sono stata violentata anche da un #magistrato italiano http   \n",
       "3                                                                                                                                                    Un altra conquista dei #PDüëÄ dare le case popolari in mano ai #Rom .üôà Risultato : #degrado e #sopprusi Si ringrazia il sindaco #Salah ü§¶‚Äç‚ôÇÔ∏è http   \n",
       "4                                                                                                                                                                          [marioafrica]: RT filippobubbico: Il benvenuto dell'Italia a #migranti. Tra oggi e domani 400 arrivi per #corridoi‚Ä¶ http   \n",
       "...                                                                                                                                                                                                                                                                                             ...   \n",
       "5595                                                                                I PDIOTI PER RIEPIRE LA PIAZZA INGAGGIANO TUTTI I MIGRANTI AFRICANI CLANDESTINI IN ITALIA  COSI PDIOTI +CLANDESTINI  69 PERSONE ü§£ü§£ü§£üòÇüòÇüòÇüòÅüòÅüòÅ AUGURI GRANDI CAZZARI COME IL BOSS CAZZARO RENZI WM5S W NUOVO GOVERNO   \n",
       "5596                                                                                                                                                         @user dica alla Fedeli che sistemati questi rom altri ne arriveranno a sfruttare la pieta' e non finiremo mai e per gl'italiani niente   \n",
       "5597                                                                                                                                                                                       Gli islamici devono adeguarsi alle tradizioni popolari e religiose dell'Italia!!! Non il contrario. http   \n",
       "5598                                                                           @user Pensi che questo √© quello che aspetta alle donne con la venuta dell'islam. Maometto era anche pedofilo. Perch√© la sinistra non da dei medievali a loro? Sinistra di gente accecata che corre verso un burrone.   \n",
       "5599                                                                                                                                                                          @user  puoi anche attivare la vibrazione, infilartelo nel culo e andare a fare in culo insieme ai tuoi amici migranti   \n",
       "\n",
       "      label  split  prediction                      logits  \\\n",
       "0         0  train           1    (-0.88551676, 0.7890035)   \n",
       "1         0  train           1     (-0.9222206, 0.8876971)   \n",
       "2         1  train           1      (-1.284241, 1.1302452)   \n",
       "3         1  train           1  (-0.16130793, 0.031392984)   \n",
       "4         0  train           0     (0.9738463, -1.1099527)   \n",
       "...     ...    ...         ...                         ...   \n",
       "5595      1  train           1    (-0.50333995, 0.3105694)   \n",
       "5596      0  train           1     (-2.8639174, 2.5761027)   \n",
       "5597      1  train           0     (1.1118305, -1.1924204)   \n",
       "5598      1   test           1     (-1.2798216, 1.0581983)   \n",
       "5599      1  train           1     (-2.1546922, 1.8866277)   \n",
       "\n",
       "                                  softmax_scores  softmax_diff  \n",
       "0      [0.15782244096304723, 0.8421775590369528]      0.684355  \n",
       "1      [0.14064807272758129, 0.8593519272724188]      0.718704  \n",
       "2      [0.08207470049519884, 0.9179252995048012]      0.835851  \n",
       "3         [0.451973296804215, 0.548026703195785]      0.096053  \n",
       "4      [0.8893185264132982, 0.11068147358670168]      0.778637  \n",
       "...                                          ...           ...  \n",
       "5595    [0.3070580631467035, 0.6929419368532965]      0.385884  \n",
       "5596  [0.004320647049271328, 0.9956793529507286]      0.991359  \n",
       "5597   [0.9092284853060056, 0.09077151469399444]      0.818457  \n",
       "5598   [0.08802273721974115, 0.9119772627802588]      0.823955  \n",
       "5599  [0.017270740338142854, 0.9827292596618572]      0.965459  \n",
       "\n",
       "[5600 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"san20_it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ffca8e-1dd1-4702-bd99-19fa2fa83e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15782244, 0.84217756])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(literal_eval(df_dict[\"san20_it\"].logits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3f3eb9-056f-4979-a0c5-865071005703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('(-0.88551676, 0.7890035)', dtype='<U24')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(df_dict[\"san20_it\"].logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787232a6-8aa9-45fd-b38b-cf7cfe79798c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15782244, 0.84217756])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "softmax(literal_eval(df_dict[\"san20_it\"].logits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e6eee8-0f28-4076-8863-429d98930519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes we have model predictions (pred_label) and uncertainty (pred_score) for each entry\n",
    "# could also do ¬ßcross-entropy for uncertainty\n",
    "# we only use the train set\n",
    "# the test set remains completely held-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "362abd89-da67-4e00-8b31-d1b08ac87619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy column for uncertainty while waiting for real results\n",
    "for dataset in df_dict:\n",
    "    df_dict[dataset][\"pred_score\"] = df_dict[dataset].label.apply(lambda x: random.uniform(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d968c7-d5aa-4f3b-a248-48dcd3e6931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has21_hi\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "has20_hi\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "for19_pt\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "ken20_en\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "  saving n = 5000 training set\n",
      "  saving n = 10000 training set\n",
      "  saving n = 20000 training set\n",
      "\n",
      "fou18_en\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "  saving n = 5000 training set\n",
      "  saving n = 10000 training set\n",
      "  saving n = 20000 training set\n",
      "\n",
      "ous19_fr\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "has19_hi\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "bas19_es\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "ous19_ar\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "san20_it\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "  saving n = 5000 training set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select top-n entries based on active learning\n",
    "# this is deterministic, so no need for multiple random seeds\n",
    "\n",
    "N_RANGE = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000]\n",
    "\n",
    "for dataset in df_dict:\n",
    "    print(dataset)\n",
    "    df_dict[dataset].sort_values(by=\"pred_score\", inplace=True)\n",
    "    for n in N_RANGE:\n",
    "        if n<len(df_dict[dataset]):\n",
    "            print(f\"  saving n = {n} training set\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214e29b-d2a6-4eb9-9754-121694c16280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate_detection_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
